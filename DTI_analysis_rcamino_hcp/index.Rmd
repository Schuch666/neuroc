---
title: "DTI Analysis using rcamino for HCP data"
author: "John Muschelli"
date: "`r Sys.Date()`"
output: 
  html_document:
    keep_md: true
    theme: cosmo
    toc: true
    toc_depth: 3
    toc_float:
      collapsed: false
    number_sections: true      
---

All code for this document is located at [here](https://raw.githubusercontent.com/muschellij2/neuroc/master/DTI_analysis_rcamino_hcp/index.R).

```{r setup, include=FALSE, message=FALSE}
library(knitr)
library(methods)
library(neurobase)
knitr::opts_chunk$set(comment = "")
```

# Resources and Goals


Much of this work has been adapted by the Tutorial for DTI analysis from ISMRM 2015: [http://camino.cs.ucl.ac.uk/index.php?n=Tutorials.ISMRM2015](http://camino.cs.ucl.ac.uk/index.php?n=Tutorials.ISMRM2015).  Also, some of the model fitting (such as using gradient information) has been taken from http://camino.cs.ucl.ac.uk/index.php?n=Tutorials.HCP.

We will show you a few steps that have been implemented in `rcamino`: `camino_pointset2scheme`, `camino_modelfit`, `camino_fa`, `camino_md`, and `camino_dteig`.  

# Data Location


# Reading in the Data
First, we download the data from HCP.  You must have your access keys set (see [Getting Data from the Human Connectome Project (HCP)](../neurohcp/index.html)).  

We will use the `neurohcp` package to download one subject data.

```{r downloading_data, echo = TRUE}
library(neurohcp)
hcp_id = "100307"
r = download_hcp_dir(
  paste0("HCP/", hcp_id, "/T1w/Diffusion"), 
  verbose = FALSE)
print(basename(r$output_files))
```

It contains 4 files:

1.  `data.nii.gz` - a 4D image of the DWI data.
2.  `nodif_brain_mask.nii.gz` - A brain mask of the DTI data
3.  `bvals` - a text file with the b-values
4.  `bvecs` - a text file with the b-vectors as the first 3 columns. 


## Creating 


As `dtifit` requires the b-values and b-vectors to be separated, and this data has b-values of $1000$ when the b-vectors is not zero.  **This is very important and you must know where your b-values and b-vectors are when doing your analyses and what units they are in.**  


```{r bvecs}
library(rcamino)
camino_set_heap(heap_size = 10000)
outfiles = r$output_files
names(outfiles) = nii.stub(outfiles, bn = TRUE)
scheme_file = camino_fsl2scheme(
  bvecs = outfiles["bvecs"], bvals = outfiles["bvals"],
  bscale = 1)
```

The imaging scheme contains measurements at b=5, b=1000, b=2000, and b=3000 s / mm^2.

## Subsetting data
By selecting a subset of the measurements, we can reduce processing time and memory requirements. Also, the high b-value shells aren't optimal for estimating the diffusion tensor. So we'll select data from the b=5 and b=1000 shells, which is still higher angular resolution than most DTI (90 directions).

If you have sufficient RAM, you can load the whole data set and extract a subset:
```{r subsetting}
camino_ver = packageVersion("rcamino")
if (camino_ver < "0.5.2") {
  source("https://neuroconductor.org/neurocLite.R")
  neuroc_install("rcamino")  
}
sub_data_list = camino_subset_max_bval(
  infile = outfiles["data"],
  schemefile = scheme_file,
  max_bval = 1500,
  verbose = TRUE) 
sub_data = sub_data_list$image
sub_scheme = sub_data_list$scheme
```

This process may take a while, and using the `RNifti` package may be quicker.

```{r subsetting_rnifti, eval = FALSE}
nim = RNifti::readNifti(outfiles["data"])
sub_data = tempfile(fileext = ".nii.gz")
sub_scheme_res = camino_subset_max_bval_scheme(
  schemefile = scheme_file, max_bval = 1500,
  verbose = TRUE)
nim = nim[,,, sub_scheme$keep_files]
RNifti::writeNifti(image = nim, file = sub_data)
sub_scheme = sub_scheme_res$scheme
rm(list = "nim");
for (i in 1:10) gc(); 
```


# Fit the diffusion tensor

```{r model_fit}
# wdtfit caminoProc/hcp_b5_b1000.Bfloat caminoProc/hcp_b5_b1000.scheme \
# -brainmask 100307/T1w/Diffusion/nodif_brain_mask.nii.gz -outputfile caminoProc/wdt.Bdouble
# 
mod_file = camino_modelfit(
  infile = sub_data, scheme = sub_scheme, 
  mask = outfiles["nodif_brain_mask"], 
  gradadj = outfiles["grad_dev"],
  model = "ldt_wtd")
```


## Getting FA vlaues

```{r making_fa}
# fa -inputfile caminoProc/wdt_dt.nii.gz -outputfile caminoProc/wdt_fa.nii.gz
fa_img = camino_fa_img(
  infile = mod_file,
  header = outfiles["nodif_brain_mask"],
  retimg = FALSE)
```





### Visualizing FA images

We want to read the FA image into `R`:
```{r fa_read}
fa_nii = readnii(fa_img)
```

In order to visualize the values, we are going to read in the mask so that we don't visualize non-brain values:
```{r mask}
mask = readnii(outfiles["nodif_brain_mask"])
```

```{r fa_hist}
hist(mask_vals(fa_nii, mask = mask), breaks = 1000)
```

Using `ortho2`, we can visualize these FA maps:
```{r ortho_fa}
ortho2(fa_nii)
```



## Getting MD vlaues

```{r making_md}
# md -inputfile caminoProc/wdt_dt.nii.gz -outputfile caminoProc/wdt_md.nii.gz
md_img = camino_md_img(
  infile = mod_file,
  header = outfiles["nodif_brain_mask"],
  retimg = FALSE)
```



### Visualizing MD images

We want to read the MD image into `R`:
```{r md_read}
md_nii = readnii(md_img)
```

```{r md_hist}
hist(mask_vals(md_nii, mask = mask), breaks = 1000)
md2 = md_nii
md2[ md2 < 0] = 0
hist(mask_vals(md2, mask = mask), breaks = 1000)
```

Using `ortho2`, we can visualize these MD maps:
```{r ortho_md}
ortho2(md_nii)
ortho2(md2)
rb_md = robust_window(md2, probs = c(0, 0.9999))
ortho2(rb_md)
```

```{r md_hist2}
hist(mask_vals(rb_md, mask = mask), breaks = 1000)
```

# Export DTs to NIfTI

Using `camino_dt2nii`, we can export the diffusion tensors into NIfTI files.  We see the result is the filenames of the NIfTI files, and that they all exist (otherwise there'd be an errors.) 
```{r nifti_mod, eval = FALSE}
# dt2nii -inputfile caminoProc/wdt.Bdouble -header 100307/T1w/Diffusion/nodif_brain_mask.nii.gz \
# -outputroot caminoProc/wdt_
mod_nii = camino_dt2nii(
  infile = mod_file,
  header = outfiles["nodif_brain_mask"])
```

```{r eigen_image, eval = FALSE}
# dteig -inputfile caminoProc/wdt.Bdouble -outputfile caminoProc/wdt_eig.Bdouble
eigen_image = camino_dteig(infile = mod_file)
```


We can read these DT images into `R` again using `readnii`, but we must set `drop_dim = FALSE` for diffusion tensor images because the pixel dimensions are zero and `readnii` assumes you want to drop "empty" dimensions

```{r, eval = FALSE}
dt_imgs = lapply(mod_nii, readnii, drop_dim = FALSE)
```

## Downloading T1 image

For image registration to a template, we will use the subject-level 
```{r}
r_t1_mask = download_hcp_file(
  file.path(
    "HCP", hcp_id, "T1w", 
    "brainmask_fs.nii.gz"), 
  verbose = FALSE
)
print(r_t1_mask)
t1_mask = readnii(r_t1_mask)
r_t1 = download_hcp_file(
  file.path(
    "HCP", hcp_id, "T1w", 
    "T1w_acpc_dc_restore.nii.gz"), 
  verbose = FALSE
)
print(r_t1)
t1 = readnii(r_t1)
brain = mask_img(t1, t1_mask)
hist(mask_vals(brain, t1_mask), breaks = 2000)
rob = robust_window(brain, probs = c(0, 0.9999), mask = t1_mask)
hist(mask_vals(rob, t1_mask), breaks = 2000)
```


## Rigid-body Registration of DTI to T1

Here we can register the FA image to the T1-weighted image using a rigid-body transformation.   We could have also used the MD image or the diffusion data directly, such as the mean over the tensors.
```{r}
library(extrantsr)
rigid = registration(
  filename = fa_img,
  template.file = rob,
  correct = FALSE,
  verbose = FALSE,
  typeofTransform = "Rigid")
rigid_trans = rigid$fwdtransforms
aff = R.matlab::readMat(rigid$fwdtransforms)
aff = aff$AffineTransform.float.3.3

double_ortho(rob, rigid$outfile)
```

Alternatively, we can also do a brain mask to brain mask transformation, which we can estimate using a last squares metric.  This should be sufficient for what we need and more robust to artifacts in the T1 or the FA map, so we'll use this transformation.

```{r mask_rig}
rigid_mask = registration(
  filename = outfiles["nodif_brain_mask"],
  template.file = r_t1_mask,
  correct = FALSE,
  typeofTransform = "Rigid",
  affMetric = "meansquares")
rigid_mask_trans = rigid_mask$fwdtransforms

aff_mask = R.matlab::readMat(rigid_mask$fwdtransforms)
aff_mask = aff_mask$AffineTransform.float.3.3

double_ortho(t1_mask, rigid_mask$outfile, NA.x = FALSE)
```

## Non-linear Registration of T1 to template

Here we will use symmetric normalization (SyN) to register the Winsorized skull-stripped brain image of the HCP subject to the Eve template.  

```{r}
library(EveTemplate)
eve_brain_fname = EveTemplate::getEvePath(what = "Brain")
eve_brain = readnii(eve_brain_fname)
nonlin = registration(
  filename = rob,
  template.file = eve_brain_fname,
  correct = FALSE,
  typeofTransform = "SyN")
double_ortho(eve_brain, nonlin$outfile)
nonlin_trans = nonlin$fwdtransforms
```


### Registering DTI to Eve

Now, we can use the transformed images from the rigid-body transformations above and apply this non-linear transformation to those FA and MD registered images.  The one problem is that the rigid-body registration interpolates the data and the non-linear registration interpolates the data.  

We can compose the transforms so that the data is only interpolated once. 

In `ants_apply_transforms`, which calls `antsApplyTransforms`, the transform list must be specified in reverse order to which they are done.  We want to perform the rigid body transformation then the non-linear registration, but need the composed list of transforms to first have the non-linear transformation then the rigid-body transformation.

Here we apply this composed transformation to the FA and MD values.

```{r composed}
composed = c(nonlin_trans, rigid_mask_trans)
fa_eve = ants_apply_transforms(
  fixed = eve_brain_fname,
  moving = fa_img,
  transformlist = composed)
double_ortho(eve_brain, fa_eve)

md_eve = ants_apply_transforms(
  fixed = eve_brain_fname,
  moving = rb_md,
  transformlist = composed)
double_ortho(eve_brain, md_eve)
```

Now we can perform this in a number of subjects and then do a population-level analysis in the template space. 



